
import cv2  #cv2: OpenCV  a computer vision library, is imported for image and video processing for ml applications.
import os  #os: The operating system module is imported to interact with the file system.
from IPython.display import display, Image  #display, Image: These modules from IPython are imported for displaying images in a Jupyter Notebook.
import numpy as np  # NumPy is imported for numerical operations.
os.chdir('D:\\aimlnew')  # chainginng the directiory  
def detectFace(net,frame,confidence_threshold=0.8):
    frameOpencvDNN=frame.copy()
    print(frameOpencvDNN.shape)
    frameHeight=frameOpencvDNN.shape[0]#return height  of the frame
    frameWidth=frameOpencvDNN.shape[1]# return width  of the frame
    #Converts the frame into a blob, which is a preprocessed image that can be fed into a deep neural network (DNN) for face detection
    blob = cv2.dnn.blobFromImage(frameOpencvDNN, 1.0, (227, 227), [124.96, 115.97, 106.13], swapRB=True, crop=False)

    net.setInput(blob)#Sets the input blob for the neural network.

    detections=net.forward()#forward propogation
    faceBoxes=[]#is initialized to store the bounding boxes of the detected faces.
    for i in range(detections.shape[2]):
        confidence = detections[0, 0, i, 2]#Extracts the confidence score of the current detection
        if confidence > confidence_threshold:
            #Extracts the coordinates of the bounding box (x1, y1, x2, y2) based on the scaled values from the neural network output.
            x1 = int(detections[0, 0, i, 3] * frameWidth)
            y1 = int(detections[0, 0, i, 4] * frameHeight)
            x2 = int(detections[0, 0, i, 5] * frameWidth)
            y2 = int(detections[0, 0, i, 6] * frameHeight)

            print(f"Confidence: {confidence}, Bounding Box: ({x1}, {y1}, {x2}, {y2})")

            faceBoxes.append([x1,y1,x2,y2])
            #creating a rectangle around the face
            cv2.rectangle(frameOpencvDNN,(x1,y1),(x2,y2),(0,255,0),int(round(frameHeight/150)),8)
    return frameOpencvDNN,faceBoxes



#File paths for face detection, age estimation, and gender prediction models are specified.
    
faceProto='opencv_face_detector.pbtxt'
faceModel='opencv_face_detector_uint8.pb'
ageProto='age_deploy.prototxt'
ageModel='age_net.caffemodel'
genderProto='gender_deploy.prototxt'
genderModel='gender_net.caffemodel'

#creating the gender list
genderList=['Male','Female']

#creating the list age which is in specific age range
ageList=['(0-2)','(4-6)','(10-20)','(25-32)','(38-43)','(48-53)','(60-100)']

#loads the pre-trained face detection model. cv2.dnn.readNet
#is used to read a neural network model from files. The
#faceModel variable contains the file path to the weights
#file (e.g., 'opencv_face_detector_uint8.pb'), and faceProto contains the file
#path to the configuration file (e.g., 'opencv_face_detector.pbtxt').
faceNet=cv2.dnn.readNet(faceModel,faceProto)
ageNet=cv2.dnn.readNet(ageModel,ageProto)
genderNet=cv2.dnn.readNet(genderModel,genderProto)

#to capturre the video
video=cv2.VideoCapture(0)
padding=20
while cv2.waitKey(1) < 0:
    
    #The hasFrame variable is a boolean indicating whether a frame was successfully read, and frame contains the actual image frame.python

    hasFrame, frame = video.read()
    if not hasFrame:
        cv2.waitKey()
        break

    cv2.imshow("Original Frame", frame)

    #detectFace function is called, passing the face 
    #detection network (faceNet) and the current frame (frame). It returns the modified image (resultImg) with bounding boxes around detected faces
    resultImg,faceBoxes=detectFace(faceNet,frame)
    
    if not faceBoxes:
        print("No face detected")
    
    for faceBox in faceBoxes:
        #converting the region of intrest in to a blob for forward process
        face=frame[max(0,faceBox[1]-padding):min(faceBox[3]+padding,frame.shape[0]-1),max(0,faceBox[0]-padding):min(faceBox[2]+padding, frame.shape[1]-1)]
        blob=cv2.dnn.blobFromImage(face,1.0,(227,227),[124.96,115.97,106.13],swapRB=True,crop=False)
        #Sets the input of the gender detection neural network (genderNet) to the face blob.
      #Performs forward propagation to obtain predictions (genderPreds) from the gender detection model.
      #gender is assigned the gender label corresponding to the highest prediction.
        genderNet.setInput(blob)
        genderPreds=genderNet.forward()
        #gender is asigned according to the prediction
        gender=genderList[genderPreds[0].argmax()]
        
        ageNet.setInput(blob)
        agePreds=ageNet.forward()
         #gender is asigned according to the prediction
        age=ageList[agePreds[0].argmax()]
        #Adds a text label (gender and age) to the original frame (resultImg) near the detected face.
        cv2.putText(resultImg,f'{gender},{age}',(faceBox[0],faceBox[1]-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,255,255),2,cv2.LINE_AA)
        cv2.imshow("Detecting age and Gender",resultImg)
        
        
        if cv2.waitKey(33) & 0xFF == ord('q'):
            break
            
cv2.destroyAllWindows()
